# LIBRARIES
# library(tidyverse)
library(tidyr)
library(dplyr)
library(tidytext)
library(stringr)
library(tm) # removing words in Spanish
# if (!require("pacman")) install.packages("pacman")
# pacman::p_load_gh("trinker/textstem")
library(textstem) # lemmatizing # to install textstem, error when installing statnet.common because my R version is 3.4, so had to work with 4.1.4 statnet.common
papers <- read.csv("cleaned_papers_all_years_simple_1000.csv", stringsAsFactors = FALSE)
datosOriginales <- read.csv("cleaned_papers_all_years_simple_1000.csv", stringsAsFactors = FALSE)
# 1.Filter columns of interest
test <- datosOriginales %>%
mutate(paper = doi) %>%
select(paper, abstract)
# Separating the abstracts by word
df_words <- test %>%
unnest_tokens(output = word, # name of the new column
input = abstract, # column that is going to be splitted
token = "words", # unit of splitting
format = "text")
# Remove English stopwords
df_words <- df_words %>%
anti_join(stop_words)
# Remove other unuseful words
bad.words <- c("use","using","used","however","based","high",
"show","shown","higher","term","can","non","first","two","three",
"one","also","may","well","among","elsevier")
bad.words <- data.frame(word = bad.words, stringsAsFactors = FALSE)
df_words <- df_words %>%
anti_join(bad.words)
# Homogenizing the American and British words
Am2Br <- read.csv(paste0(path_dict_tools,"Am2Br.csv"), stringsAsFactors = FALSE)
names(Am2Br) <- c("Am","Br")
# Function created by RJ
Americanizing <- function(words,Am2Br){
Am <- gsub("[[:punct:]]", "", Am2Br$Am)
Br <- gsub("[[:punct:]]", "", Am2Br$Br)
word.word <- gsub("[[:punct:]]", "",tolower(unlist(strsplit(words, " "))))
unlist(lapply(word.word,function(x){
ind.match <- match(x,Br)
# print(ind.match)
if (is.na(ind.match)==FALSE){
Am.word <- Am[ind.match]
}else{
Am.word <- x
}
# print(Am.word)
return(Am.word)
}))
}
system.time(
Rep.Word <- Americanizing(df_words$word, Am2Br)
)
# Homogenizing the American and British words
Am2Br <- read.csv("Am2Br.csv", stringsAsFactors = FALSE)
names(Am2Br) <- c("Am","Br")
# Function created by RJ
Americanizing <- function(words,Am2Br){
Am <- gsub("[[:punct:]]", "", Am2Br$Am)
Br <- gsub("[[:punct:]]", "", Am2Br$Br)
word.word <- gsub("[[:punct:]]", "",tolower(unlist(strsplit(words, " "))))
unlist(lapply(word.word,function(x){
ind.match <- match(x,Br)
# print(ind.match)
if (is.na(ind.match)==FALSE){
Am.word <- Am[ind.match]
}else{
Am.word <- x
}
# print(Am.word)
return(Am.word)
}))
}
#Am2Br2 <- apply(X = Am2Br, MARGIN = 2, FUN = function(x){gsub("'", "", x)})
system.time(
Rep.Word <- Americanizing(df_words$word, Am2Br)
)
# Adding the new words
df_words$NewWords <- Rep.Word
# Removing Numbers
# http://stla.github.io/stlapblog/posts/Numextract.html
indexRemoveNumbers <- str_detect(df_words$NewWords, "\\-*\\d+\\.*\\d*")
df_words2 <- df_words[!indexRemoveNumbers,]
df_words2 <- df_words2 %>%
select(paper, NewWords)
datos <- df_words2
# Find the abbreviation 'km' and replace it with 'kilometer'
indexkm <- which(datos$NewWords == 'km')
datos$NewWords[indexkm] <- 'kilometer'
# Find the abbreviation 'tdrs' and replace it with 'tdr'
indextdrs <- which(datos$NewWords == 'tdrs')
datos$NewWords[indextdrs] <- 'tdr'
# Find the abbreviation 'tdrs' and replace it with 'tdr'
indexbeh <- which(datos$NewWords == 'behavioral')
datos$NewWords[indexbeh] <- 'behavior'
# Remove the abbreviations 'eg' and 'ie'
# indexeg <- datos$NewWords == 'eg'
# datos2 <- datos[!indexeg, ]
abbr <- data.frame(NewWords = c("eg","ie"), stringsAsFactors = FALSE)
datos2 <- datos %>%
anti_join(abbr)
spanishSW <- data.frame(NewWords = stopwords(kind = "sp"), stringsAsFactors = FALSE)
datos2 <- datos2 %>%
anti_join(spanishSW)
# LEMMATIZING
system.time(prueba <- lemmatize_words(datos2$NewWords))
# Adding the root to the dataframe datos2
datos2$word <- prueba
prueba1 <- which(datos2$NewWords=='gps')
datos2$word[prueba1] <- 'gps'
# Remove the rest of the abbreviations 'gp' meaning 'grazed pixels'
prueba2 <- which(datos2$word=='gp')
datos3 <- datos2[datos2$word!='gp',]
# FINALLY
# TIDY DATA
datosFinales <- datos3 %>%
select(paper, word)
# Save data in the project
write.csv(x = datosFinales, file = "cleaned_TidyData_TopicModeling.csv", row.names = FALSE)
library(tidyverse)
library(tm)
library(tidytext)
library(topicmodels)
library(gridExtra)
library(grid)
library(parallel)
# READ DATA ---------------------------------------------------------------
datosFinales <- read.csv("cleaned_TidyData_TopicModeling.csv", stringsAsFactors = FALSE)
# DOCUMENT TERM MATRIX ----------------------------------------------------
datosFreq <- datosFinales %>%
# Obtain word frequencies
count(paper, word)
# Document Term Matrix
datos_dtm <- datosFreq %>%
# Specify the token
cast_dtm(document = paper, term = word, value = n)
modk5 <- LDA(x = datos_dtm, k = 5, method = "Gibbs", control = list(alpha=1, delta= 0.1, seed = 10005))
modk <- modk5
saveRDS(modk,file = "Topics.rds")
library(dplyr)
library(tidyr) # spread
library(ggplot2)
# library(tm)
library(tidytext)
library(topicmodels)
library(ggwordcloud)
library(cowplot)
library(viridis)
library(RColorBrewer)
# library(gridExtra)
papers <- read.csv(file = "cleaned_papers_all_years_simple_1000.csv",stringsAsFactors = FALSE)
data_decade <- read.csv(file = "cleaned_papers_all_years_simple_1000.csv",stringsAsFactors = FALSE)
data_decade_summ <- data_decade %>% select(doi,pubyear)
modk <- readRDS(file = "Topics.rds")
# 1. Topic as a mixture of words
# beta: the probability of that term being generated from that topic.
papers_beta <- tidytext::tidy(modk, matrix = "beta")
# papers_beta
head(papers_beta)
# topic_1 <- papers_beta %>% filter(topic == 1 & beta > 0.001) %>%  select(term,beta)
topic_sample <- papers_beta %>% filter(beta > 0.005) #%>%  select(term,beta)
table_topic <- topic_sample %>%
mutate(angle = 45 * sample(-2:2, n(), replace = TRUE, prob = c(1, 2, 4, 2, 1)))
# set.seed(42)
# pdf(paste0("./Rocio/Plots/wordcloud3.pdf"), width = 7, height = 7)
option_par <- "D"
plot_w <- ggplot(table_topic, aes(label = term, size = beta, angle = angle,
color = beta)) +
geom_text_wordcloud_area(rm_outside = TRUE, eccentricity = 1) + #area_corr_power = 1,
scale_size_area(max_size = 30) +
theme_minimal()+ facet_wrap(~topic) +
scale_color_viridis(direction = -1, option = option_par)
height_par <- 26
width_par <- 26
ggsave(plot=plot_w,filename="wordcloud_topics.pdf", height=height_par, width = width_par, units = "in")
modk <- LDA(x = datos_dtm, k = 10, method = "Gibbs", control = list(alpha=1, delta= 0.1, seed = 10005))
saveRDS(modk,file = "Topics.rds")
# 1. Topic as a mixture of words
# beta: the probability of that term being generated from that topic.
papers_beta <- tidytext::tidy(modk, matrix = "beta")
# papers_beta
head(papers_beta)
# topic_1 <- papers_beta %>% filter(topic == 1 & beta > 0.001) %>%  select(term,beta)
topic_sample <- papers_beta %>% filter(beta > 0.005) #%>%  select(term,beta)
table_topic <- topic_sample %>%
mutate(angle = 45 * sample(-2:2, n(), replace = TRUE, prob = c(1, 2, 4, 2, 1)))
# set.seed(42)
# pdf(paste0("./Rocio/Plots/wordcloud3.pdf"), width = 7, height = 7)
option_par <- "D"
plot_w <- ggplot(table_topic, aes(label = term, size = beta, angle = angle,
color = beta)) +
geom_text_wordcloud_area(rm_outside = TRUE, eccentricity = 1) + #area_corr_power = 1,
scale_size_area(max_size = 30) +
theme_minimal()+ facet_wrap(~topic) +
scale_color_viridis(direction = -1, option = option_par)
height_par <- 26
width_par <- 26
ggsave(plot=plot_w,filename="wordcloud_topics.pdf", height=height_par, width = width_par, units = "in")
papers_gamma <- tidytext::tidy(modk, matrix = "gamma")
head(papers_gamma)
# Long format to short format
papers_gamma_short <- spread(papers_gamma, key = topic, value = gamma)
names(papers_gamma_short) <- c("document", paste("Topic", names(papers_gamma_short)[-1]))
View(papers_gamma_short)
papers_summ <- cbind.data.frame(doi = as.character(papers_gamma_short$document),
topic_max = apply(papers_gamma_short[,2:ncol(papers_gamma_short)],1,which.max),
gamma_max = apply(papers_gamma_short[,2:ncol(papers_gamma_short)],1,max))
# now we have to count and make a data frame
papers_summ$topic_max <- as.factor(papers_summ$topic_max)
topics_count <- papers_summ %>% select(doi, topic_max) %>%
group_by(topic_max) %>%
summarise(n())
colnames(topics_count) <- c("Topic","num_papers")
topics_count <- topics_count %>%
# arrange(desc(num_papers))
arrange(num_papers)
View(topics_count)
ggplot(topics_count, aes(x=Topic,y=num_papers)) +
geom_bar(stat="identity",position="identity") +
coord_flip() +
xlab("Topic") + ylab("Number of articles") +
theme_cowplot() + theme(text = element_text(size=20),axis.text.y = element_text(size=16))
topics_count <- topics_count %>%
# arrange(desc(num_papers))
arrange(num_papers)
topics_count$Topic <- factor(topics_count$Topic,levels=topics_count$Topic)
ggplot(topics_count, aes(x=Topic,y=num_papers)) +
geom_bar(stat="identity",position="identity") +
coord_flip() +
xlab("Topic") + ylab("Number of articles") +
theme_cowplot() + theme(text = element_text(size=20),axis.text.y = element_text(size=16))
